{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T05:16:53.039333Z",
     "start_time": "2019-07-03T05:16:52.698085Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.mllib.clustering import PowerIterationClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T05:17:05.865957Z",
     "start_time": "2019-07-03T05:17:05.861192Z"
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to compute pairwise (Euclidean) distances between data points\n",
    "def Distance_Computing(entry, DF, p=2):\n",
    "    # Obtain the coordinate of the point\n",
    "    point = entry[1]\n",
    "    # Compute the pairwise distances between this point and other points in the data\n",
    "    dist = np.empty([DF.shape[0], ], dtype='float64')\n",
    "    for i in range(DF.shape[0]):\n",
    "        dist[i] = np.linalg.norm(point - DF[i,:], ord=p)\n",
    "    return (entry[0], dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T05:17:15.965562Z",
     "start_time": "2019-07-03T05:17:15.959579Z"
    }
   },
   "outputs": [],
   "source": [
    "# A helper function: Transform the pairwise distances to the required similarity entry of pyspark PIC\n",
    "def Affinity(entry, sigma=1):\n",
    "    ID = entry[0]\n",
    "    dists = entry[1]\n",
    "    tuple_lst = []\n",
    "    for i in range(ID, len(dists)):\n",
    "        tuple_lst.append((ID, i, np.exp(-dists[i]/(2*sigma))))\n",
    "    return tuple_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T05:17:27.975917Z",
     "start_time": "2019-07-03T05:17:27.948923Z"
    }
   },
   "outputs": [],
   "source": [
    "def Power_Iteration_Clustering(X, K, Adj=False, Lp_norm=2, sigma=1, max_Iter=20):\n",
    "    '''\n",
    "    Input: \n",
    "        X : [n_samples, n_samples] numpy array if adj=True, or, a [n_samples_a, n_features] array otherwise;\n",
    "        K: int, The number of clusters;\n",
    "        adj: boolean, Indicating whether the adjacency matrix is pre-computed. Default: False;\n",
    "        Lp_norm: int, Indicating which L^p norm is using. Default: 2;\n",
    "        sigma: float, The variance for the Gaussian (aka RBF) kernel. Default: 1;\n",
    "        max_Iter: int, Maximum number of iterations of the PIC algorithm. Default: 20.\n",
    "    Output:\n",
    "        cluster labels: A [n_samples, ] numpy array,\n",
    "        node ids: A list with length \"n_samples\".\n",
    "    '''\n",
    "    # Setting up PySpark Context\n",
    "    conf = SparkConf()\n",
    "    sc = SparkContext(conf=conf)\n",
    "    \n",
    "    if Adj:\n",
    "        # Concatenate the point ID to the last column of the array\n",
    "        X1 = np.concatenate((X, np.array(range(X.shape[0]), ndmin=2).T), axis=1)\n",
    "        data = sc.parallelize(X1.tolist())\n",
    "        # Manipulate the RDD such that each entry is a tuple of the form (ID, distance_list)\n",
    "        Adj_matRDD = data.map(lambda x: (int(x[len(x)-1]), x[:(len(x)-1)]))\n",
    "    else:\n",
    "        X1 = np.concatenate((X, np.array(range(X.shape[0]), ndmin=2).T), axis=1)\n",
    "        data = sc.parallelize(X1.tolist())\n",
    "        data = data.map(lambda x: (int(x[len(x)-1]), x[:(len(x)-1)]))\n",
    "        # Compute the pairwise distances between points\n",
    "        Adj_matRDD = data.map(lambda item: Distance_Computing(item, DF=X, p=Lp_norm))\n",
    "    \n",
    "    # Transform the affinity matrix such that each element in the list has the form (i, j, s_{ij})\n",
    "    A_RDD = Adj_matRDD.flatMap(lambda item: Affinity(item, sigma=sigma))\n",
    "    # Cluster the data into two classes using PowerIterationClustering\n",
    "    model = PowerIterationClustering.train(A_RDD, K, 100)\n",
    "    \n",
    "    cluster_id = model.assignments().collect()\n",
    "    sc.stop()\n",
    "    IDs = [k.id for k in cluster_id]\n",
    "    clusters = [k.cluster for k in cluster_id]\n",
    "    # Sort the cluster label list based on the ascending order of their IDs\n",
    "    IDs_sorted = sorted(IDs)\n",
    "    clusters_sorted = np.array(clusters)[np.argsort(IDs)]\n",
    "    \n",
    "    return clusters_sorted, IDs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T05:17:38.785415Z",
     "start_time": "2019-07-03T05:17:38.776637Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Generate a toy dataset\n",
    "    N = 500\n",
    "    mean1 = [3, 0]\n",
    "    cov1 = [[0.3, 0], [0, 0.8]]  # diagonal covariance\n",
    "    mean2 = [-1, 4]\n",
    "    cov2 = [[0.2, 0], [0, 1]]  # diagonal covariance\n",
    "    a = np.random.multivariate_normal(mean1, cov1, N)\n",
    "    b = np.random.multivariate_normal(mean2, cov2, N)\n",
    "    big_data = np.concatenate((a, b), axis=0)\n",
    "    labels = np.concatenate((np.repeat(0, N), np.repeat(1, N)))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(x=big_data[:, 0], y=big_data[:, 1], c=labels, s=2)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    PIC_labels, IDs = Power_Iteration_Clustering(big_data, K=2, Lp_norm=2, sigma=1, max_Iter=20)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(x=big_data[:, 0], y=big_data[:, 1], c=PIC_labels, s=2)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-03T05:17:44.893Z"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
